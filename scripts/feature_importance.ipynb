{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null/Empty counts for each column:\n",
      "timestamp_call_key                        0\n",
      "retailer_code                             0\n",
      "serial                                    0\n",
      "reason                                    0\n",
      "mos                                       0\n",
      "resolved                                  0\n",
      "no_of_accounts_with_syf_13_march       3400\n",
      "account_balance_13_march             113782\n",
      "delinquency_history_13_march              0\n",
      "account_open_date_13_march                0\n",
      "account_status_13_march                   0\n",
      "card_activation_status_13_march           0\n",
      "eservice_ind_13_march                     0\n",
      "ebill_enrolled_status_13_march            0\n",
      "auto_pay_enrolled_status_13_march         0\n",
      "no_of_accounts_with_syf_18_march       3402\n",
      "account_balance_18_march             101829\n",
      "delinquency_history_18_march              0\n",
      "account_open_date_18_march                0\n",
      "account_status_18_march                   0\n",
      "card_activation_status_18_march           0\n",
      "eservice_ind_18_march                     0\n",
      "ebill_enrolled_status_18_march            0\n",
      "auto_pay_enrolled_status_18_march         0\n",
      "date_of_call                              0\n",
      "time_of_call                              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_counts = df.isnull().sum()\n",
    "print(\"Null/Empty counts for each column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timestamp_call_key', 'retailer_code', 'serial', 'reason', 'mos',\n",
       "       'resolved', 'no_of_accounts_with_syf_13_march',\n",
       "       'account_balance_13_march', 'delinquency_history_13_march',\n",
       "       'account_open_date_13_march', 'account_status_13_march',\n",
       "       'card_activation_status_13_march', 'eservice_ind_13_march',\n",
       "       'ebill_enrolled_status_13_march', 'auto_pay_enrolled_status_13_march',\n",
       "       'no_of_accounts_with_syf_18_march', 'account_balance_18_march',\n",
       "       'delinquency_history_18_march', 'account_open_date_18_march',\n",
       "       'account_status_18_march', 'card_activation_status_18_march',\n",
       "       'eservice_ind_18_march', 'ebill_enrolled_status_18_march',\n",
       "       'auto_pay_enrolled_status_18_march', 'date_of_call', 'time_of_call'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50).to_csv('short_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting codes and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'mos' column by spaces and get the first element of each split to find the starting codes\n",
    "starting_codes = df['cleaned_mos'].str.split().str[0]\n",
    "\n",
    "# Count the occurrences of each starting code\n",
    "starting_codes_count = starting_codes.value_counts()\n",
    "\n",
    "print(\"Starting codes and their counts:\")\n",
    "print(starting_codes_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' starts with \"mn\" and extract the next two codes\n",
    "mn_sequences = df['cleaned_mos'][df['cleaned_mos'].str.startswith('mn')].str.split().str[1:3]\n",
    "\n",
    "# Convert the list of codes to a string for easier counting\n",
    "mn_sequences_str = mn_sequences.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Count unique sequences and their occurrences\n",
    "mn_unique_counts = mn_sequences_str.value_counts()\n",
    "\n",
    "print(\"Unique cases where 'mos' starts with 'mn' and their counts:\")\n",
    "print(mn_unique_counts)\n",
    "print(f\"Total unique cases: {mn_unique_counts.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' starts with \"mn mn RS\"\n",
    "mn_mn_rs_rows = df[df['mos'].str.startswith('mn mn RS')]\n",
    "\n",
    "# Save these rows to a CSV file\n",
    "mn_mn_rs_rows.to_csv('mn_mn_rs_rows.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(mn_mn_rs_rows)} rows to 'mn_mn_rs_rows.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' starts with \"mn mn mn\"\n",
    "mn_mn_mn_rows = df[df['mos'].str.startswith('mn mn mn')]\n",
    "\n",
    "# Save these rows to a CSV file\n",
    "mn_mn_mn_rows.to_csv('mn_mn_mn_rows.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(mn_mn_mn_rows)} rows to 'mn_mn_mn_rows.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' starts with \"mm\" and extract the next two codes\n",
    "mm_sequences = df['mos'][df['mos'].str.startswith('mm')].str.split().str[1:3]\n",
    "\n",
    "# Convert the list of codes to a string for easier counting\n",
    "mm_sequences_str = mm_sequences.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Count unique sequences and their occurrences\n",
    "mm_unique_counts = mm_sequences_str.value_counts()\n",
    "\n",
    "print(\"Unique cases where 'mos' starts with 'mm' and their counts:\")\n",
    "print(mm_unique_counts)\n",
    "print(f\"Total unique cases: {mm_unique_counts.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' starts with \"mm\" followed by \"mm mm\"\n",
    "mm_mm_mm_rows = df[df['mos'].str.startswith('mm mm mm')]\n",
    "\n",
    "# Save these rows to a CSV file\n",
    "mm_mm_mm_rows.to_csv('mm_mm_mm_rows.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(mm_mm_mm_rows)} rows to 'mm_mm_mm_rows.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of \"IA\" followed by \"BA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'mos' has \"IA BA\" sequence and count them\n",
    "ia_ba_count = df[df['cleaned_mos'].str.contains('IA BA')].shape[0]\n",
    "\n",
    "print(f'\"IA\" followed by \"BA\" occurs {ia_ba_count} times.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows containing IA, BA, and TR in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regular expressions to find rows where IA, BA, and TR appear in any order\n",
    "rows_with_ia_ba_tr = df[df['mos'].str.contains('IA') & df['mos'].str.contains('BA') & df['cleaned_mos'].str.contains('TR')].shape[0]\n",
    "\n",
    "print(f'Rows containing IA, BA, and TR in any order: {rows_with_ia_ba_tr}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows ending with TR, divided by \"resolved\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of rows ending with TR and resolved status\n",
    "rows_ending_with_tr_resolved = df[(df['mos'].str.endswith('TR')) & (df['resolved'] == 'resolved')].shape[0]\n",
    "\n",
    "# Count of rows ending with TR and floor status\n",
    "rows_ending_with_tr_floor = df[(df['mos'].str.endswith('TR')) & (df['resolved'] == 'floor')].shape[0]\n",
    "\n",
    "print(f'Rows ending with TR and resolved: {rows_ending_with_tr_resolved}')\n",
    "print(f'Rows ending with TR and floor: {rows_ending_with_tr_floor}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Resolved values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'resolved' column\n",
    "resolved_counts = df['resolved'].value_counts()\n",
    "\n",
    "print(\"Counts of unique values in the 'resolved' column:\")\n",
    "print(resolved_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing most impactful codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mos_sequence(mos):\n",
    "    # Split the sequence into codes\n",
    "    codes = mos.split()\n",
    "    \n",
    "    # Remove everything up to the first \"IA\" including that \"IA\"\n",
    "    if \"IA\" in codes:\n",
    "        first_ia_index = codes.index(\"IA\") + 1  # Move past the first \"IA\"\n",
    "        codes = codes[first_ia_index:]\n",
    "    \n",
    "    # Now, remove consecutive \"IAs\" that follow immediately after\n",
    "    while codes and codes[0] == \"IA\":\n",
    "        codes.pop(0)  # Keep removing \"IA\" until we encounter a different code\n",
    "    \n",
    "    # Remove all \"BA\" and \"TR\" codes from what remains\n",
    "    codes = [code for code in codes if code not in (\"BA\", \"TR\")]\n",
    "    \n",
    "    return ' '.join(codes)\n",
    "\n",
    "# Apply the adjusted cleaning function to the 'mos' column\n",
    "df['cleaned_mos'] = df['mos'].apply(clean_mos_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tansw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Coefficient  Absolute Coefficient\n",
      "39      RS    10.079166             10.079166\n",
      "47      TN    -5.391877              5.391877\n",
      "42      TB    -5.041593              5.041593\n",
      "29      PC     4.310617              4.310617\n",
      "36      RC     4.281609              4.281609\n",
      "..     ...          ...                   ...\n",
      "59      eS     0.045563              0.045563\n",
      "30      PI    -0.044125              0.044125\n",
      "69      iT     0.024342              0.024342\n",
      "56      eB    -0.021953              0.021953\n",
      "48      TP    -0.009868              0.009868\n",
      "\n",
      "[78 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df and clean_mos_sequence_v2 as before and df['cleaned_mos_v2'] is ready\n",
    "\n",
    "# Encoding the 'cleaned_mos_v2' sequences\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "X = vectorizer.fit_transform(df['cleaned_mos'].values)\n",
    "\n",
    "# Encoding the 'resolved' column (1 for 'floor', 0 otherwise)\n",
    "y = (df['resolved'] == 'floor').astype(int)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = model.coef_[0]\n",
    "\n",
    "# Creating a DataFrame to display feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sorting features by absolute importance\n",
    "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "feature_importance_sorted = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "# Displaying the top influential features\n",
    "print(feature_importance_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_sorted.to_csv('logistic_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tansw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Importance\n",
      "75      mt    0.313048\n",
      "39      RS    0.142852\n",
      "35      Pd    0.055902\n",
      "33      PT    0.055104\n",
      "14      FI    0.038958\n",
      "..     ...         ...\n",
      "56      eB    0.000000\n",
      "27      OC    0.000000\n",
      "51      Te    0.000000\n",
      "45      TE    0.000000\n",
      "62      iF    0.000000\n",
      "\n",
      "[78 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming the data preparation steps are the same\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "X = vectorizer.fit_transform(df['cleaned_mos'].values)\n",
    "y = (df['resolved'] == 'floor').astype(int)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "ml_feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "ml_feature_importance_df = ml_feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(ml_feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML - Linear Support Vector Machine (SVM) with Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tansw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\tansw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Coefficient  Absolute Coefficient\n",
      "39      RS     2.589569              2.589569\n",
      "47      TN    -1.857296              1.857296\n",
      "10      DE     1.735202              1.735202\n",
      "42      TB    -1.667842              1.667842\n",
      "4       BL     1.517775              1.517775\n",
      "36      RC     1.513772              1.513772\n",
      "14      FI     1.468856              1.468856\n",
      "15      FM     1.456148              1.456148\n",
      "29      PC     1.453462              1.453462\n",
      "66      iP     1.447207              1.447207\n",
      "9       CT     1.419551              1.419551\n",
      "25      NU     1.406764              1.406764\n",
      "44      TD    -1.397162              1.397162\n",
      "68      iS     1.357198              1.357198\n",
      "43      TC    -1.301690              1.301690\n",
      "21      LS     1.284685              1.284685\n",
      "71      me     1.281550              1.281550\n",
      "8       CD     1.246708              1.246708\n",
      "35      Pd     1.234342              1.234342\n",
      "46      TL    -1.228536              1.228536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "X = vectorizer.fit_transform(df['cleaned_mos'].values)\n",
    "y = (df['resolved'] == 'floor').astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm_model = LinearSVC(max_iter=10000)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = svm_model.coef_[0]\n",
    "feature_importance_svm = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "feature_importance_svm['Absolute Coefficient'] = feature_importance_svm['Coefficient'].abs()\n",
    "feature_importance_svm_sorted = feature_importance_svm.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance_svm_sorted.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_svm_sorted.to_csv('svm_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Classifier (with L2 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tansw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Feature  Coefficient  Absolute Coefficient\n",
      "39      RS     2.026062              2.026062\n",
      "47      TN    -1.779387              1.779387\n",
      "42      TB    -1.631254              1.631254\n",
      "10      DE     1.562741              1.562741\n",
      "4       BL     1.497005              1.497005\n",
      "14      FI     1.473131              1.473131\n",
      "66      iP     1.452929              1.452929\n",
      "29      PC     1.452430              1.452430\n",
      "36      RC     1.449985              1.449985\n",
      "15      FM     1.419483              1.419483\n",
      "68      iS     1.388167              1.388167\n",
      "44      TD    -1.387542              1.387542\n",
      "43      TC    -1.277350              1.277350\n",
      "21      LS     1.266163              1.266163\n",
      "71      me     1.263555              1.263555\n",
      "8       CD     1.239214              1.239214\n",
      "35      Pd     1.228700              1.228700\n",
      "46      TL    -1.214026              1.214026\n",
      "60      eY     1.141885              1.141885\n",
      "20      LC     1.063283              1.063283\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)\n",
    "X = vectorizer.fit_transform(df['cleaned_mos'].values)\n",
    "y = (df['resolved'] == 'floor').astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ridge_model = RidgeClassifier()\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Extracting feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "coefficients = ridge_model.coef_[0]\n",
    "feature_importance_ridge = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "feature_importance_ridge['Absolute Coefficient'] = feature_importance_ridge['Coefficient'].abs()\n",
    "feature_importance_ridge_sorted = feature_importance_ridge.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "print(feature_importance_ridge_sorted.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_ridge_sorted.to_csv('ridge_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "df_logreg = pd.read_csv('logistic_features.csv')\n",
    "df_svm = pd.read_csv('svm_features.csv')\n",
    "df_ridge = pd.read_csv('ridge_features.csv')\n",
    "\n",
    "# Normalize the Coefficients\n",
    "df_logreg['Normalized Coefficient'] = df_logreg['Coefficient'] / df_logreg['Absolute Coefficient'].max()\n",
    "df_svm['Normalized Coefficient'] = df_svm['Coefficient'] / df_svm['Absolute Coefficient'].max()\n",
    "df_ridge['Normalized Coefficient'] = df_ridge['Coefficient'] / df_ridge['Absolute Coefficient'].max()\n",
    "\n",
    "# Rank the Features based on Absolute Normalized Coefficients\n",
    "df_logreg['Rank'] = df_logreg['Absolute Coefficient'].rank(method='min', ascending=False)\n",
    "df_svm['Rank'] = df_svm['Absolute Coefficient'].rank(method='min', ascending=False)\n",
    "df_ridge['Rank'] = df_ridge['Absolute Coefficient'].rank(method='min', ascending=False)\n",
    "\n",
    "# Merge the dataframes on 'Feature', including normalized coefficients\n",
    "df_merged = pd.merge(df_logreg[['Feature', 'Rank', 'Normalized Coefficient']], df_svm[['Feature', 'Rank', 'Normalized Coefficient']], on='Feature', how='inner', suffixes=('_logreg', '_svm'))\n",
    "df_merged = pd.merge(df_merged, df_ridge[['Feature', 'Rank', 'Normalized Coefficient']], on='Feature', how='inner', suffixes=('', '_ridge'))\n",
    "\n",
    "# Calculate Average Rank and Average Coefficient\n",
    "df_merged['Average Rank'] = df_merged[['Rank_logreg', 'Rank_svm', 'Rank']].mean(axis=1)\n",
    "df_merged['Average Coefficient'] = df_merged[['Normalized Coefficient_logreg', 'Normalized Coefficient_svm', 'Normalized Coefficient']].mean(axis=1)\n",
    "\n",
    "# Prepare the final dataframe\n",
    "df_final = df_merged[['Feature', 'Average Rank', 'Average Coefficient']].sort_values(by='Average Rank')\n",
    "\n",
    "# Saving the \"median\" list with average coefficient to a CSV\n",
    "df_final.to_csv('features_median.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
